{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwYp_PIQjQxv"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spacy\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBvZraf2jYIG"
      },
      "outputs": [],
      "source": [
        "# Downloading the Spam SMS Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "!unzip /content/smsspamcollection.zip\n",
        "!rm /content/readme\n",
        "!rm !rm /content/smsspamcollection.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1HceqWHjYKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97be0291-f129-4899-cb83-f610473d698e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-16 01:46:15--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-05-16 01:46:15--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2023-05-16 01:48:54 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "### Downloading the GloVe embeddings database\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip /content/glove.6B.zip\n",
        "!rm -rf /content/glove.6B.zip\n",
        "!rm /content/glove.6B.100d.txt\n",
        "!rm /content/glove.6B.200d.txt\n",
        "!rm /content/glove.6B.300d.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAdzZHGZl3C5"
      },
      "outputs": [],
      "source": [
        "text = []\n",
        "label = []\n",
        "with open(\"/content/SMSSpamCollection\") as f:\n",
        "    lines=[]\n",
        "    for line in f.readlines():\n",
        "      #Splitting based on tab\n",
        "      words = line.split(\"\\t\")\n",
        "      words[1]=words[1][:-2]\n",
        "      #Labelling Spam as 1\n",
        "      if words[0]==\"spam\":\n",
        "        words[0]=1\n",
        "      else:\n",
        "        words[0]=0\n",
        "      label.append(words[0])\n",
        "      text.append(words[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRuld-70mCdT"
      },
      "outputs": [],
      "source": [
        "# Creating a Pandas Dataframe\n",
        "sms = pd.DataFrame(zip(text, label), columns = [\"Text\", \"Label\"])\n",
        "sms['Text_Length'] = sms[\"Text\"].str.len()\n",
        "\n",
        "#Converting all strings to lower\n",
        "sms[\"Text\"] = sms[\"Text\"].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzOJgFYCmTPx"
      },
      "outputs": [],
      "source": [
        "spacy_tokenizer = spacy.load('en_core_web_sm')\n",
        "def tokenize (text):\n",
        "  #Removing punctuations\n",
        "  text = re.sub(r'[^\\w\\s]',' ',text)\n",
        "\n",
        "  #Removing non ascii\n",
        "  text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
        "\n",
        "  #Removing multiple spaces\n",
        "  text = re.sub(' +',' ',text)\n",
        "\n",
        "  #Using Spacy tokenizer on the text\n",
        "  doc = spacy_tokenizer(text)\n",
        "  return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5mH9RpmmsEl"
      },
      "outputs": [],
      "source": [
        "# Tokenizing the text sms.\n",
        "sms[\"Tokenized_Text\"] = sms['Text'].apply(tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw0txfuIm29P"
      },
      "outputs": [],
      "source": [
        "def load_GloVe_embeddings(glove_file):\n",
        "  dic = {}\n",
        "  #Creating embedding dictonary\n",
        "  with open(glove_file) as f:\n",
        "    for line in f:\n",
        "      w_lines = line.split()\n",
        "      word = w_lines[0]\n",
        "      dic[word] = np.array(w_lines[1:],dtype=np.float32)\n",
        "  return dic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the glove embeddings\n",
        "word_embeds = load_GloVe_embeddings(\"/content/glove.6B.50d.txt\")"
      ],
      "metadata": {
        "id": "V_nUMtEqTS2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-O4DeKRnjL8"
      },
      "outputs": [],
      "source": [
        "def embed_text(tokenized_text, word_embeddings, max_text_length=20, embedding_size = 50):\n",
        "  embeds = np.zeros((max_text_length,50))\n",
        "  wordsfound=0\n",
        "  for token_idx in range(len(tokenized_text)):\n",
        "    word = tokenized_text[token_idx].text\n",
        "    if(word in word_embeddings):\n",
        "      embeds[wordsfound] = (word_embeddings[word][:embedding_size])\n",
        "      wordsfound+=1;\n",
        "    if wordsfound == max_text_length:\n",
        "      break\n",
        "  return embeds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmOFLtqbn-2w"
      },
      "outputs": [],
      "source": [
        "sms[\"Embedded_Text\"] = sms[\"Tokenized_Text\"].apply(lambda x: embed_text(x,word_embeds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtOWOOv_jYPF"
      },
      "outputs": [],
      "source": [
        "#Creating Dataloader Class\n",
        "class load_dataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.x = X\n",
        "        self.y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.x[idx]\n",
        "        lab = self.y[idx]\n",
        "        return item,lab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6fuH7svjYRr"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, num_layers, hidden_size=256):\n",
        "      #Setting up Model structure\n",
        "        super(RNN,self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(vocab_size,hidden_size,num_layers,batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_size,2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      #Defining a forwad pass\n",
        "        hid = torch.zeros(self.num_layers,x.size(0),self.hidden_size,dtype = torch.float32)\n",
        "        out,_ = self.rnn(x,hid)\n",
        "        out = self.fc(out[:,-1])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a function to check accuracy\n",
        "\n",
        "def acc_check(model,loader):\n",
        "  cor = 0\n",
        "  samp = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for val,lab in loader:\n",
        "      val = val.to(torch.float32)\n",
        "      score = model(val)\n",
        "      _,pred = score.max(1)\n",
        "      cor+=(pred==lab).sum()\n",
        "      samp+=pred.size(0)\n",
        "  print(f'Got {cor} / {samp} with accuracy {float(cor)/float(samp)*100:.2f}')\n",
        "  model.train()\n",
        "  return\n",
        "\n"
      ],
      "metadata": {
        "id": "YusSjC--wdjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRlz7uqbjYUC"
      },
      "outputs": [],
      "source": [
        "def train_model(num_epochs, train_loader, model, criterion, optimizer):\n",
        "    for epoch in range(num_epochs):\n",
        "      for idx,(data,label) in enumerate(train_loader):\n",
        "        #Calculating Scores\n",
        "        data = data.to(torch.float32)\n",
        "\n",
        "        #Running model on data\n",
        "        mark = model(data)\n",
        "\n",
        "        #Calculating losses\n",
        "        loss = criterion(mark,label)\n",
        "\n",
        "        #Setting\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #Backwards Propogation\n",
        "        loss.backward()\n",
        "\n",
        "        #Doing the descent\n",
        "        optimizer.step()\n",
        "\n",
        "      #Checking accuracy for each epoch\n",
        "      acc_check(model,train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rap_LDx1jYW4"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Test-Train Split\n",
        "train, test = train_test_split(sms, test_size=0.2, random_state=42, shuffle=True)\n",
        "train = train.reset_index()\n",
        "test = test.reset_index()\n",
        "train_ = load_dataset(train[\"Embedded_Text\"],train[\"Label\"])\n",
        "test_ = load_dataset(test[\"Embedded_Text\"],test[\"Label\"])\n",
        "train_loader = DataLoader(train_,5)\n",
        "test_loader = DataLoader(test_,5)\n",
        "\n",
        "#Model\n",
        "model = RNN(50,2,256)\n",
        "\n",
        "#Setting up Hyper Paramters\n",
        "alpha = 0.0001\n",
        "epochs = 3\n",
        "crit = nn.CrossEntropyLoss()\n",
        "opti  = torch.optim.Adam(model.parameters(),lr=alpha)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "evc1ulcMnwV2",
        "outputId": "3dc06776-5ae3-490b-adde-ff0ce4fb2106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(50, 256, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(epochs,train_loader,model,crit,opti)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qig0jsae1Uyb",
        "outputId": "f1ccae27-42ac-4e4d-e6d6-534967691400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 4309 / 4459 with accuracy 96.64\n",
            "Got 4338 / 4459 with accuracy 97.29\n",
            "Got 4347 / 4459 with accuracy 97.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_check(model,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK7PRWpS77kX",
        "outputId": "1e2635c4-1c36-4905-9344-f28aee25a034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 1072 / 1115 with accuracy 96.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "torch.save(model,\"/content/model.pth\")"
      ],
      "metadata": {
        "id": "YLjwZ8mx8gbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = torch.load(\"/content/model.pth\")"
      ],
      "metadata": {
        "id": "LphmdCSKG9iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model1)"
      ],
      "metadata": {
        "id": "blLVnvG8HHOE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "RNN_for_Spam_Classification.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}